#import packages#
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
import random 
import itertools

N_true_inputs=2 #Number of true inputs
N_false_inputs=8 #Number of false inputs
n_obs=100 #Nubmer of observations:Estimation
n_pred=100 #Nubmer of observations:Prediction
error_sd=0.5 #Standar deviation of the error term

# Total number of inputs (simply set intercept = 0)
p = N_true_inputs + N_false_inputs

# Total number of observations
n = n_obs + n_pred

#Parameter vector:
#True inputs have coefficient 1
#False inputs have a zero (or very small) coefficient ###########bug is corrected 
beta = np.asmatrix(np.zeros((p,1)))
beta[0:N_true_inputs,:] = 1

#Simulate the data: Note that X is a matrix, not a data frame
np.random.seed(1)
X = np.asmatrix(np.random.rand(n,p))
epsilon = np.asmatrix(error_sd*np.random.normal(0,size=(n,1))) 
y = X*beta + epsilon
X = pd.DataFrame(X) 
y = pd.DataFrame(y)

# Pack the data into a dataframe
DF = pd.concat([pd.DataFrame(X),pd.DataFrame(y)],axis=1) 
new_names_true = ['x_true_'+str(i) for i in range(1,N_true_inputs+1)] 
new_names_false = ['x_false_'+str(i) for i in range(1,N_false_inputs+1)] 
names = new_names_true + new_names_false + ['y']
DF.columns = names
 
# Randomly draw n_obs observations
train_index = random.sample(range(0,n),100)
train_index.sort()

DF_estimation = DF.loc[train_index,:] 
DF_prediction = DF.drop(index=train_index)

#get the estimation and prediction data
X_est,y_est = DF_estimation.iloc[:,0:p],DF_estimation['y'] 
X_pred,y_pred = DF_prediction.iloc[:,0:p],DF_prediction['y']

# Fit model on feature_set and calculate rsq_adj
def processSubset(X,y,feature_set):
    regr = sm.OLS(y,X[list(feature_set)]).fit() 
    rsq_adj = regr.rsquared_adj
    return {"model":regr, "rsq_adj":rsq_adj}

# Pull out predictors we still need to process 
def forward(predictors,X,y):
    remaining_predictors = [p for p in X.columns if p not in predictors]
    results = []
    for p in remaining_predictors:
        results.append(processSubset(X,y,predictors+[p]))
        # Wrap everything up in a nice dataframe
    models = pd.DataFrame(results)
    # Choose the model with the highest rsq_adj
    best_model = models.loc[models['rsq_adj'].idxmax()]
    # Return the best model, along with models other information return best_model
    return best_model
models_fwd = pd.DataFrame(columns=["rsq_adj", "model"]) 
predictors = []

for i in range(1,len(X_est.columns)+1): 
     models_fwd.loc[i] = forward(predictors,X_est,y_est)
     predictors = models_fwd.loc[i]["model"].model.exog_names
     print(predictors)
print(models_fwd.loc[4, "model"].summary()) 


#backwoard step selection#
#定义一个新的变量#
def backward(predictors,X,y):
    results = []
    for combo in itertools.combinations(predictors, len(predictors)-1):
        results.append(processSubset(X,y,combo))
# Wrap everything up in a nice dataframe
    models = pd.DataFrame(results)
# Choose the model with the maximum rsq_adj
    best_model = models.loc[models['rsq_adj'].idxmax()]
# Return the best model, along with some other useful informati return best_model
    return best_model

models_bwd = pd.DataFrame(columns=["rsq_adj","model"], index=range(1,len(X_est.columns)))
predictors = list(X_est.columns)                    
#for i in range(1,len(X_est.columns)+1): 
while len(predictors)>1:
    print(len(predictors))
    models_bwd.loc[len(predictors)-1] = backward(predictors,X_est,y_est)
    predictors = models_bwd.loc[len(predictors)-1]["model"].model.exog_names
    print(predictors)
print(models_bwd.loc[4, "model"].summary())   



#ridge regression#
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import scale 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV
from sklearn.metrics import mean_squared_error
from sklearn import linear_model



alphas = [np.exp(i/10) for i in range(-150,50,2)] 
coefs = []
for a in alphas:
    ridge = linear_model.Ridge(alpha=a, fit_intercept=False) 
    ridge.fit(DF_estimation.iloc[:,0:p],DF_estimation['y']) 
    ridge.coef_ = np.insert(ridge.coef_,0,ridge.intercept_) 
    coefs.append(ridge.coef_)
dim_coef = np.array(coefs).shape 
print(dim_coef)
 
plt.subplot(1, 2, 1) 
plt.scatter(range(0,dim_coef[1]),coefs[0]) 
plt.axhline(y=0, c="g", ls="-", lw=2) 
plt.title('least_regularized')
plt.subplot(1, 2, 2) 
plt.scatter(range(0,dim_coef[1]),coefs[100-1],c='slategrey') 
plt.axhline(y=0, c='coral', ls="-", lw=2) 
plt.title('most_regularized')
plt.show()
 
#这个地方修正过了#
from sklearn.linear_model import RidgeCV
ridge_cv = RidgeCV(alphas=alphas,store_cv_values=True)
ridge_cv.fit(DF_estimation.iloc[:,0:p],DF_estimation['y']) 
lambda_min = ridge_cv.alpha_

ridge_cv_pred = ridge_cv.predict(DF_prediction.iloc[:,0:p]) 
print(np.mean((ridge_cv_pred-DF_prediction['y'])**2))

r = linear_model.Ridge(alpha=lambda_min, fit_intercept=False) 
ridge = r.fit(DF_estimation.iloc[:,0:p],DF_estimation['y']) 
ridge_pred = ridge.predict(DF_prediction.iloc[:,0:p]) 
print(ridge.coef_)
 
 
 
#the lasso#
from sklearn.linear_model import Lasso 
from sklearn.linear_model import LassoCV 
l_coefs = []
for a in alphas:
    lasso = linear_model.Lasso(alpha=a, fit_intercept=False) 
    lasso.fit(X, y)
    l_coefs.append(lasso.coef_)


    
#e未必能被识别，可以用近似值来替代或者怎样#
ax = plt.gca()
ax.semilogx(alphas,l_coefs,basex=2.7) 
ax.set_xticklabels([-6,-5,-4,-3,-2,-1,0,1,2]) 
ax.set_xlim(ax.get_xlim()[::-1])
plt.ylabel('weights')
plt.title('Lasso coefficients as a function of the regularization') 
plt.axis('tight')
plt.show()

#get the estimation and prediction data
X_est,y_est = DF_estimation.iloc[:,0:p],DF_estimation['y'] 
X_pred,y_pred = DF_prediction.iloc[:,0:p],DF_prediction['y']
lasso_cv = LassoCV(alphas=alphas,cv=2).fit(X_est,y_est) 
alphas_ = lasso_cv.alphas_
mse_mean = np.mean(lasso_cv.mse_path_,axis=1)
mse_std = np.std(lasso_cv.mse_path_)
 
plt.figure().set_size_inches(8, 6) 
plt.semilogx(alphas_,mse_mean)

ax = plt.gca() 
ax.semilogx(alphas,l_coefs,basex=2.7) 
ax.set_xticklabels([-6,-5,-4,-3,-2,-1,0,1,2]) 
ax.set_xlim(ax.get_xlim()[::-1]) # reverse axis
plt.figure().set_size_inches(8, 6)
plt.semilogx(alphas_,mse_mean)

# plot error lines showing +/- std. errors of the mse
std_error = mse_std / np.sqrt(1)#这个地方取代了之前的k，请自行定义#
plt.semilogx(alphas_, mse_mean + std_error, 'b--') 
plt.semilogx(alphas_, mse_mean - std_error, 'b--')
# alpha=0.2 controls the translucency of the fill color
plt.fill_between(alphas_,mse_mean+std_error,mse_mean-std_error,alpha=0.2)
plt.ylabel('CV mse +/- std error') 
plt.xlabel('alphas_') 
plt.axhline(np.min(mse_mean), ls='--', color='.5') 
plt.show()

lambda_min = lasso_cv.alpha_ 
print("lambda_min:", lambda_min)

lasso_cv_pred = lasso_cv.predict(DF_prediction.iloc[:,0:p]) 
print(np.mean((lasso_cv_pred-DF_prediction['y'])**2))

l = linear_model.Lasso(alpha=lambda_min, fit_intercept=True) 
lasso = l.fit(DF_estimation.iloc[:,0:p],DF_estimation['y']) 
lasso_pred = lasso.predict(DF_prediction.iloc[:,0:p]) 
print(lasso.coef_)
 
 
