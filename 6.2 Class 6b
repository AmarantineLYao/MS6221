#import packages#
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
import random 
import itertools

N_true_inputs=2 #Number of true inputs
N_false_inputs=8 #Number of false inputs
n_obs=100 #Nubmer of observations:Estimation
n_pred=100 #Nubmer of observations:Prediction
error_sd=0.5 #Standar deviation of the error term

# Total number of inputs (simply set intercept = 0)
p = N_true_inputs + N_false_inputs

# Total number of observations
n = n_obs + n_pred

#Parameter vector:
#True inputs have coefficient 1
#False inputs have a zero (or very small) coefficient ###########bug is corrected 
beta = np.asmatrix(np.zeros((p,1)))
beta[0:N_true_inputs,:] = 1

#Simulate the data: Note that X is a matrix, not a data frame
np.random.seed(1)
X = np.asmatrix(np.random.rand(n,p))
epsilon = np.asmatrix(error_sd*np.random.normal(0,size=(n,1))) 
y = X*beta + epsilon
X = pd.DataFrame(X) 
y = pd.DataFrame(y)

# Pack the data into a dataframe
DF = pd.concat([pd.DataFrame(X),pd.DataFrame(y)],axis=1) 
new_names_true = ['x_true_'+str(i) for i in range(1,N_true_inputs+1)] 
new_names_false = ['x_false_'+str(i) for i in range(1,N_false_inputs+1)] 
names = new_names_true + new_names_false + ['y']
DF.columns = names
 
# Randomly draw n_obs observations
train_index = random.sample(range(0,n),100)
train_index.sort()

DF_estimation = DF.loc[train_index,:] 
DF_prediction = DF.drop(index=train_index)

#get the estimation and prediction data
X_est,y_est = DF_estimation.iloc[:,0:p],DF_estimation['y'] 
X_pred,y_pred = DF_prediction.iloc[:,0:p],DF_prediction['y']

# Fit model on feature_set and calculate rsq_adj
def processSubset(X,y,feature_set):
    regr = sm.OLS(y,X[list(feature_set)]).fit() 
    rsq_adj = regr.rsquared_adj
    return {"model":regr, "rsq_adj":rsq_adj}

# Pull out predictors we still need to process 
def forward(predictors,X,y):
    remaining_predictors = [p for p in X.columns if p not in predictors]
    results = []
    for p in remaining_predictors:
        results.append(processSubset(X,y,predictors+[p]))
        # Wrap everything up in a nice dataframe
    models = pd.DataFrame(results)
    # Choose the model with the highest rsq_adj
    best_model = models.loc[models['rsq_adj'].idxmax()]
    # Return the best model, along with models other information return best_model
    return best_model
models_fwd = pd.DataFrame(columns=["rsq_adj", "model"]) 
predictors = []

for i in range(1,len(X_est.columns)+1): 
     models_fwd.loc[i] = forward(predictors,X_est,y_est)
     predictors = models_fwd.loc[i]["model"].model.exog_names
     print(predictors)
print(models_fwd.loc[4, "model"].summary()) 


#backwoard step selection#
#定义一个新的变量#
def backward(predictors,X,y):
    results = []
    for combo in itertools.combinations(predictors, len(predictors)-1):
        results.append(processSubset(X,y,combo))
# Wrap everything up in a nice dataframe
    models = pd.DataFrame(results)
# Choose the model with the maximum rsq_adj
    best_model = models.loc[models['rsq_adj'].idxmax()]
# Return the best model, along with some other useful informati return best_model
    return best_model

models_bwd = pd.DataFrame(columns=["rsq_adj","model"], index=range(1,len(X_est.columns)))
predictors = list(X_est.columns)                    
#for i in range(1,len(X_est.columns)+1): 
while len(predictors)>1:
    print(len(predictors))
    models_bwd.loc[len(predictors)-1] = backward(predictors,X_est,y_est)
    predictors = models_bwd.loc[len(predictors)-1]["model"].model.exog_names
    print(predictors)
print(models_bwd.loc[4, "model"].summary())   



#ridge regression#
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import scale 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV
from sklearn.metrics import mean_squared_error
from sklearn import linear_model



alphas = [np.exp(i/10) for i in range(-150,50,2)] 
coefs = []
for a in alphas:
    ridge = linear_model.Ridge(alpha=a, fit_intercept=False) 
    ridge.fit(DF_estimation.iloc[:,0:p],DF_estimation['y']) 
    ridge.coef_ = np.insert(ridge.coef_,0,ridge.intercept_) 
    coefs.append(ridge.coef_)
dim_coef = np.array(coefs).shape 
print(dim_coef)
 
plt.subplot(1, 2, 1) 
plt.scatter(range(0,dim_coef[1]),coefs[0]) 
plt.axhline(y=0, c="g", ls="-", lw=2) 
plt.title('least_regularized')
plt.subplot(1, 2, 2) 
plt.scatter(range(0,dim_coef[1]),coefs[100-1],c='slategrey') 
plt.axhline(y=0, c='coral', ls="-", lw=2) 
plt.title('most_regularized')
plt.show()
 
#这个地方修正过了#
from sklearn.linear_model import RidgeCV
ridge_cv = RidgeCV(alphas=alphas,store_cv_values=True)
ridge_cv.fit(DF_estimation.iloc[:,0:p],DF_estimation['y']) 
lambda_min = ridge_cv.alpha_

ridge_cv_pred = ridge_cv.predict(DF_prediction.iloc[:,0:p]) 
print(np.mean((ridge_cv_pred-DF_prediction['y'])**2))

r = linear_model.Ridge(alpha=lambda_min, fit_intercept=False) 
ridge = r.fit(DF_estimation.iloc[:,0:p],DF_estimation['y']) 
ridge_pred = ridge.predict(DF_prediction.iloc[:,0:p]) 
print(ridge.coef_)
 
 
 
